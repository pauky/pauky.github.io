<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="杨润炜"><meta name="copyright" content="杨润炜"><title>Cwalker</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.2.0'
} </script><meta name="generator" content="Hexo 6.2.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">杨润炜</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">132</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">29</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Cwalker</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Cwalker</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/a/92.html">了解极大似然估计</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-20</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>了解极大似然估计原理</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li><p>因为某些场景的整体数据量过大，要分析其中某些情况出现的概率，就可以用极大似然估计，这样的情况在机器学习算法中频繁出现。下面用一个简单的例子来解析它：<br>  <img src="http://img.blog.csdn.net/20170528002827749" alt="Minion"><br>  这就是它最直观的原理讲解了。<br>  极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。<br>  然而在一些复杂的条件下，是很难通过直观的方式获得答案的，这时候理论分析就尤为重要了，这也是学者们为何要提出最大似然估计的原因。</p>
<p>  由于样本集中的样本都是独立同分布，可以只考虑一类样本集D，来估计参数向量θ。记已知的样本集为：</p>
<p>  <img src="http://img.blog.csdn.net/20170528003138251">  </p>
<p>  似然函数（linkehood function）：联合概率密度函数<img src="http://img.blog.csdn.net/20170528003212360">称为相对于<img src="http://img.blog.csdn.net/20170528003218392">的θ的似然函数。</p>
<p>  <img src="http://img.blog.csdn.net/20170528003223845"><br>  如果<img src="http://img.blog.csdn.net/20170528003231366">是参数空间中能使似然函数<img src="http://img.blog.csdn.net/20170528003236220">最大的θ值，则<img src="http://img.blog.csdn.net/20170528003231366">应该是“最可能”的参数值，那么<img src="http://img.blog.csdn.net/20170528003231366">就是θ的极大似然估计量<br>  因为事件已经发生，所以要使它发生的概率<img src="http://img.blog.csdn.net/20170528003212360">最大，这就可以转换为求极大似然函数的最大值，接下来就是求对数（简化求解过程），求导求最值。求解大概流程如下：<br>  求最大似然估计量<img src="http://img.blog.csdn.net/20170528003231366">的一般步骤：</p>
<p>  （1）写出似然函数；</p>
<p>  （2）对似然函数取对数，并整理；</p>
<p>  （3）求导数；</p>
<p>  （4）解似然方程。</p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了极大似然估计怎样利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。这也是平时生活中一些抽样检查的原理依据，比如人口普查；还有用在机器学习算法里的一些概率形式的数据也是用这种方法获得的。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="http://blog.csdn.net/zengxiantao1994/article/details/72787849">极大似然估计详解</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/90.html">动手学深度学习（笔记）- 卷积神经网络</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>1.什么是卷积神经网络</li>
<li>2.为什么要使用卷积神经网络</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li><p>卷积神经网络除了包含普通神经网络的输入输出层、激励层、隐含层之外，还有两种特殊的结构：卷积计算层和池化层；<br>  卷积计算层：用于线性乘积 求和；<br>  了解卷积神经网络，首先要了解什么是卷积：<br>  对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做<strong>内积</strong>（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源；举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据：<br>  <img src="http://img.blog.csdn.net/20160702215705128" alt="Minion"><br>  中间滤波器filter与数据窗口做内积，其具体计算过程则是：4*0 + 0*0 + 0*0 + 0*0 + 0*1 + 0*1 + 0*0 + 0*1 + -4*2 &#x3D; -8;<br>  池化层：取区域平均或最大；<br>  下面举个简单的池化例子：<br>  <img src="http://img.blog.csdn.net/20160703121026432" alt="Minion"><br>  上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4;</p>
</li>
<li><p>卷积层的两个要点：<strong>局部连接（Local Connection）</strong>和<strong>权值共享（Weight Sharing）</strong>;<br>  局部连接和权值共享降低了参数量，使训练复杂度大大下降，并减轻了过拟合。同时权值共享还赋予了卷积网络对平移的容忍性;<br>  池化层的降采样输出参数量，并赋予了模型对轻度形变的容忍性，提高了模型的泛化能力;<br>  相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少。</p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了对图像识别更友好有效的卷积神经网络的基本组成结构和实现原理。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zh.gluon.ai/chapter_convolutional-neural-networks/cnn-scratch.html">动手学深度学习-卷积神经网络</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/v_july_v/article/details/51812459">通俗理解卷积神经网络</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/91.html">认识循环神经网络</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>了解循环神经网络</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li><p>先从基本的循环神经网络说起，它只包含一个隐含层，结构如下：<br>  <img src="http://upload-images.jianshu.io/upload_images/2256672-cf18bb1f06e750a4.jpg" alt="Minion"><br>  与之前所说的全连接神经网络的区别在于隐含层多了带箭头权重为w的圆圈；<br>  图中各部分解析如下：<br>  x是一个向量，它表示<strong>输入层</strong>的值；<br>  s是一个向量，它表示<strong>隐藏层</strong>的值；<br>  U是输入层到隐藏层的<strong>权重矩阵</strong>；<br>  o也是一个向量，它表示<strong>输出层</strong>的值；<br>  V是隐藏层到输出层的<strong>权重矩阵</strong>；<br>  W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重；</p>
<p>  从图右侧计算流程看出，这个网络在t时刻接收到输入xt之后，隐藏层的值是st，输出值是ot。关键一点是，st的值不仅仅取决于xt，还取决于st-1。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：</p>
<p>  <img src="https://upload-images.jianshu.io/upload_images/1667471-55cef3bda3b88ee9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/206" alt="Minion"><br>  将第二个式子不断代入第一个式，得到：<br>  <img src="https://upload-images.jianshu.io/upload_images/1667471-a3efd4e7588c38fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/553" alt="Minion"><br>  <strong>循环神经网络</strong>的输出值   ，是受前面历次输入值的影响的，这就是<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<p>  接下来简单认识下<strong>双向循环神经网络</strong>，大体流程如下图：<br>  <img src="http://upload-images.jianshu.io/upload_images/2256672-039a45251aa5d220.png" alt="Minion"><br>  从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值取决于A2和A2’，两者分别是正向计算和反向计算的结果，所以双向循环神经网络可以向前和向后看任意个输入值。</p>
<p>  最后再来看看<strong>深度循环神经网络</strong>，它是在基本循环神经网络的单层隐含层转变为多隐含层，而且隐含层之间存在输入输出联系，如下图所示：<br>  <img src="http://upload-images.jianshu.io/upload_images/2256672-df137de8007c3d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt="Minion"></p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了几种循环神经网络的基本实现原理，以及它适合自然语言处理和机器翻译的原因。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zybuluo.com/hanbingtao/note/541458">零基础入门深度学习(5) - 循环神经网络</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/89.html">动手学深度学习（笔记）- 监督学习之正则化的多层感知机</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-16</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="正则化的多层感知机"><a href="#正则化的多层感知机" class="headerlink" title="正则化的多层感知机"></a>正则化的多层感知机</h1><h2 id="学习的内容"><a href="#学习的内容" class="headerlink" title="学习的内容"></a>学习的内容</h2><ul>
<li>1.理解L2范数正则化原理</li>
<li>2.了解正则化的多层感知机结构</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li>以线性回归中的梯度下降法为例。假设要求的参数为θ，LOSS是我们的损失函数，那么线性回归的代价函数如下：</li>
</ul>
<p>$$<br>J (θ)&#x3D;LOSS<br>$$</p>
<p>其中loss是损失函数是以θ求导的导数函数，那么在梯度下降法中，最终用于迭代计算参数 θ 的迭代式为：</p>
<p>$$<br>θj :&#x3D;θj − loss<br>$$</p>
<p>其中 α 是学习率，m是训练数据数量。上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面的样子：</p>
<p>$$<br>θj :&#x3D;θj (1−{αλ&#x2F;m}  )−loss<br>$$</p>
<p>其中λ  就是正则化参数。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代， θj   都要先乘以一个小于1的因子，从而使得 θ j   不断减小，因此总得来看， θ是不断减小的。在神经网络中，参数越小抗扰动能力越强，可以极大程度地降低噪声数据的影响，所以能防止过拟合的情况出现。</p>
<ul>
<li>正则化的多层感知机结构如下：<br><img src="https://zh.gluon.ai/_images/backprop.svg" alt="Minion"><br>其中各个节点的解析如下：<br>x表示输入；<br>o表示当前参数模型在正则化前的输出；<br>y表示当前输入对应的实际输出数据；<br>z表示输入数据与第一个参数的运算结果；<br>h表示神经网络隐藏层；<br>W (1)和W(2)都是所要求的神经网络参数；<br>s表示参数的L2范数正则化；<br>L表示正则化前的损失运算值；<br>J表示加入正则后的损失运算值；<br>此结构图清晰地展示了正则化的多层感知机训练流程。</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>理解了正则化解决过拟合的原理，并且从正则化多层感知机结构图中更加定性地了解到神经网络的训练过程。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zh.gluon.ai/chapter_supervised-learning/backprop.html">动手学深度学习-正向传播与反向传播</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/88.html">动手学深度学习（笔记）- 监督学习之正则化</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="监督学习之正则化"><a href="#监督学习之正则化" class="headerlink" title="监督学习之正则化"></a>监督学习之正则化</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>1.什么是训练误差和泛化误差</li>
<li>2.了解欠拟合与过拟合</li>
<li>3.了解正则化</li>
</ul>
<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><ul>
<li><p>训练误差是机器学习模型在训练数据集上表现出的误差，测试误差是模型在测试集上表现出的误差，而泛化误差指的是模型在任意一个测试数据样本上表现出的误差的期望值，泛化误差越小，模型效果才越符合实际要求；</p>
</li>
<li><p><strong>欠拟合</strong>：机器学习模型无法得到较低训练误差，往往是由于选用的模型参数过少，导致无法拟合现有训练数据的某些特征；<br>  <strong>过拟合</strong>：机器学习模型的训练误差远小于其在测试数据集上的误差，可能是由于选用的模型参数过多，或者是训练数据远少于数据的特征数量；</p>
</li>
<li><p>正则化是解决过拟合的方法之一，正则化是对较大的绝对值的参数实施了一个惩罚规则，防止一些噪声数据对某些特征影响过大而使模型参数过度拟合这些影响；除了正则化，增加训练数据和选取合适的模型（如：过度拟合时可适当减少参数数量）来解决过度拟合问题，但如果是选用的特征是无法减少的，比如考虑房价的规律，那影响的因素就很多，而正则化可以让我们既不用减少特征参数，又能让所有的影响因素都考虑到模型里，而且让影响小的因素占的权重小。</p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了模型训练过程中可能出现的一些异常情况和其中的一些优化方法。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/87.html">动手学深度学习（笔记）- 监督学习之多层感知机</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>1.多层感知机-从0开始</li>
<li>2.多层感知机 — 使用Gluon</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li>本节所介绍的多层感知机是包含一个隐含层的神经网络模型，这里的多层指的是在原先输入和输出层之间增加了一个隐含层，而且本节使用的隐含层是一个非线性函数relu，使得原本只具有单一线性的模型具备了拟合非线性特征的功能，模型效果也有了显著的提升；在查阅资料中发现多层感知机所用到训练算法与单层感知机有一些明显的区别，训练时候需要对损失函数求导来更新模型参数，因为单层感知机只需要对一个函数里的参数求导，而多层则需要对多个函数求导，然后利用链式法则求出各层网络的梯度项，之后再根据当前迭代结果，逆向更新各层参数和阈值，这也称反向传播算法，普遍用在训练神经网络；这个算法过程在gluon提供的函数里已经实现了。</li>
<li>利用gluon提供的模型层可以很方便搭建神经网络，用Dense可以作为输入输出层和隐含层。</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了神经网络训练的反向传播算法原理；了解到了线性模型可以作为简单的神经网络单层感知机，而且可以用来构建复杂多层的神经网络；不过很多事物规律都不是简单的线性，一些非线性的引入可以提高模型对实际数据的拟合度。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/86.html">动手学深度学习（笔记）- 监督学习之多类逻辑回归</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="多类逻辑回归"><a href="#多类逻辑回归" class="headerlink" title="多类逻辑回归"></a>多类逻辑回归</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>1.多类逻辑回归-从0开始</li>
<li>2.多类逻辑回归-使用Gluon</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li><p>基本训练流程与上一节的线性回归一样，不同之处在于前者输出结果是多维度的，而后者是单个维度的，前者也有不同的损失函数和输出前的处理等区别。下面主要列举两者的不同之处：<br>  softmax函数：用在对输出结果的处理，将k维向量转换到另一个k维向量上，使得所有维度都在[0,1]之间，并且维度之和为1，这样就将输出结果归一化为各个类别的概率值；<br>  交叉熵损失函数：由于softmax使线性模型的参数偏导在大部分时候取值过小，训练时参数更新过于缓慢，所以不使用先前讨论的方差损失函数，而使用交叉熵的形式；当训练结果的概率分布与实际概率分布越相近，代价值越小；</p>
</li>
<li><p>提供了优化Softmax和交叉熵数值计算不稳定的函数；不需要对输入数据及参数作维度定义，只需要调出相应的模型层（线性模型用Dense层）来接收数据输入即可；与上节不同的是，此处输入数据是多维矩阵，需要用Flatten层将数据转成特定维度的矩阵。</p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解了多分类逻辑回归用到的softmax和交叉熵，这些在神经网络中也会经常被用到。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zh.gluon.ai/chapter_supervised-learning/softmax-regression-scratch.html">动手学深度学习-多类逻辑回归</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/85.html">动手学深度学习（笔记）—监督学习之线性回归</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="监督学习—线性回归"><a href="#监督学习—线性回归" class="headerlink" title="监督学习—线性回归"></a>监督学习—线性回归</h1><h2 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h2><ul>
<li>1.NDArray和autograd处理数据和自动求导</li>
<li>2.线性回归-从0开始</li>
<li>3.线性回归-使用gluon</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li>矩阵运算是机器学习的核心基础之一，算法处理数据都是矩阵的运算、求导过程。NDArray提供了操作符、替换、截取等操作，进行对矩阵数据的存储和变换，autograd则提供了更新模型参数所需的自动求导功能。</li>
<li>模型训练流程：<br>  <strong>获取数据集</strong>：用线性函数加噪声的方式生产训练数据，噪声是为了模拟现实情况的数据，所以使用了能反应普遍规律的正态分布；<br>  <strong>定义模型</strong>：y  ̂    &#x3D;X w+b；<br>  <strong>确定损失函数</strong>：模型的方差：∑ i&#x3D;1 n (y  ̂     i −y i ) 2 ，这里方差的体现了实际数据与直线的欧氏距离之和，显然距离最小的时候，模型最能反映实际数据了；<br>  <strong>优化算法</strong>：采用梯度下降使损失最小化，首先是确定模型参数的初始值、梯度下降的学习率及训练迭代次数，然后以学习率为单位逐渐减小参数的偏导数，使平均损失值逐渐减小，迭代完成后，就可以得到此次训练的最优参数值和模型；查阅资料时发现还可以使用最小二乘法求解，利用导数为零时损失最小的原理，在参数的偏导公式中代入训练数据求出具体参数值；不过由于矩阵的逆计算量大而且存在数值不稳定的情况，所以迭代法（如：梯度下降）更适合实际训练。</li>
<li>gluon引入了一些抽象概念和更便捷的数据处理方法，有加载数据的data模块，模型层（如：线性模型的Dense），内置方差计算和梯度下降法运算。gluon使模型训练更快，实现起来更简洁。</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>理解机器学习最简单的线性模型原理及实现，了解gluon的一些基本用法和抽象概念。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zh.gluon.ai/">动手学深度学习</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/suibianshen2012/article/details/51393733">最小二乘法与梯度下降</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/84.html">动手学深度学习（笔记）-机器学习简介</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-02-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/ml/">ml</a></span><div class="content"><h1 id="机器学习简介"><a href="#机器学习简介" class="headerlink" title="机器学习简介"></a>机器学习简介</h1><h2 id="笔记序言"><a href="#笔记序言" class="headerlink" title="笔记序言"></a>笔记序言</h2><p>接下来会有一波关于深度学习入门资源（<a target="_blank" rel="noopener" href="https://zh.gluon.ai/">动手学深度学习</a>）的笔记，建议有兴趣的朋友到其资源网站上查阅。</p>
<h2 id="学习的内容"><a href="#学习的内容" class="headerlink" title="学习的内容"></a>学习的内容</h2><ul>
<li>1.什么是机器学习、机器学习的流程</li>
<li>2.机器学习最简要素</li>
<li>3.了解监督学习</li>
<li>4.了解无监督学习</li>
<li>5.需要与环境交互的机器学习</li>
</ul>
<h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><ul>
<li><p>1.机器学习就是用&#x3D;&#x3D;数据&#x3D;&#x3D;来进行计算机编程的过程。通过选定的&#x3D;&#x3D;模型&#x3D;&#x3D;算法，并配置合适的&#x3D;&#x3D;参数&#x3D;&#x3D;，将大量数据输入迭代，达到预期效果。下图可以很好地展示了机器学习的流程：<br><img src="https://zh.gluon.ai/_images/ml-loop.png" alt="Minion"></p>
</li>
<li><p>2.四个要素：<br>  <strong>数据</strong>：量越多越好，而且要具备所要训练的各种维度信息，维度信息一般都需要人工标注；<br>  <strong>模型</strong>：分析统计数据维度信息的算法；<br>  <strong>损失函数</strong>：比较模型结果与真实结果好坏来衡量模型好坏的方法，包含用训练数据比较得出的训练误差和用测试数据比较得出的测试误差；<br>  <strong>优化算法</strong>：调整参数，逐渐最小化损失的方法。</p>
</li>
<li><p>3.<strong>监督学习</strong>：通过大量包含输入x和对应输出y的数据，估算条件概率P(y|x)，典型特点是每个输入都具有人工给定的目标值。<br>  几种监督学习：<br>  回归分析：输入单一或多个的连续或离散的变量，得到连续的结果；<br>  分类：能够得到预先规定好的结果类别，结果往往是所有分类都有一个判定概率，这里选定概率最大的作为最优解。比如文字识别（OCR）、色情识别；<br>  标注：预测所有可能的非互斥分类，可以为被标注对象的定义多个标签；<br>  搜索与排序：搜集有效的数据，并进行有效权重排序的过程；<br>  推荐系统：包含用户个性化定制的搜索与排序；<br>  序列学习：输入与输出都可能是任务长度的序列，而且需要处理各个输入之间可能存在的联系，如语音识别、机器翻译等。</p>
</li>
<li><p>4.<strong>无监督学习</strong>：没有为特定的输入给出输出值，只有一个确定的目标，需要利用合适的模型，使其分析数据集的相关维度信息，达到目标效果，例如聚类、子空间估计、表征学习、生成对抗网络等。</p>
</li>
<li><p>5.一般我们用都是拿数据到特定环境下训练的离线学习模式，即不会在训练时与正在变化的环境做对比交互，生成对抗网络虽然也有对比，但对比的对象是确定的不变的。与环境交互时，会观察环境中的++动作++做出应对策略，用赏对罚错的形式来提升这个智能体的“智力”。</p>
</li>
</ul>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>了解基本的机器学习流程及方法领域</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zh.gluon.ai/">动手学习深度学习</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/a/83.html">关于nginx的几点小事</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2017-05-23</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/tech/">tech</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/nginx/">nginx</a></span><div class="content"><p>##平滑升级nginx<br>在之前的文章说明了<a target="_blank" rel="noopener" href="https://www.yangrunwei.com/a/34.html">重新编译nginx</a>，里面就涉及到平滑升级的问题，及不停服地更新nginx版本或者给nginx编译进新的模块。nginx提供了一种方法，即给nginx当前进程发送一个USR2信号量，启用一个新的nignx（新版本或新编译的nginx），新的请求将流入nginx新进程，而旧的nignx会继续处理之前的请求，直到请求结束，旧进程退出，新进程完全接替nignx的工作。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -USR2 `cat /run/nginx.pid`</span><br></pre></td></tr></table></figure>
<p>然而，很多人应该会得到一个结果：完全没有效果！！！<br>为什么呢？查nginx&#x2F;error.log日志会发现，抛出了这样的错误：<br>10690#0: execve() failed while executing new binary process “nginx” (2: No such file or directory)<br>这告诉我们，找不到nginx这个命令。这是因为我们一开始启动时就是依赖的是环境变量，但这时候nginx需要的是一个绝对的路径来找到nginx命令所在，所以出错了。<br>解决方法是：启动时要使用绝对路径。如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/nginx -c /etc/nginx.conf</span><br></pre></td></tr></table></figure>
<p>以后就可以顺利用USR2来平滑升级nginx了。</p>
<p>##访问限制<br>可以通过nginx提供的ngx_http_limit_conn_module模块来实现访问限制。<br>先从实例入手吧。如果想限制<a href="http://www.yangrunwei.com这个域名每个ip同时只能有一个请求，可以这样：">www.yangrunwei.com这个域名每个ip同时只能有一个请求，可以这样：</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">limit_conn_zone $binary_remote_addr zone=addr:10m;</span><br><span class="line">server &#123;</span><br><span class="line">    location /www.yangrunwei.com/ &#123;</span><br><span class="line">        limit_conn addr 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>limit_conn_zone用来指定一个存储请求的区域，请求的索引可以是ip或者server_name，语法为：limit_conn_zone $variable zone&#x3D;name:size; 这里使用$binary_remote_addr，即请求ip的二进制，大小为10m，可以同时容纳16多万请求。<br>limit_conn：指定每个给定键值的最大同时连接数，当超过这个数字时被返回503 (Service Temporarily Unavailable)错误。</p>
<p>##生成请求唯一标识<br>某些api服务，需要清晰展示请求从进入到返回的整个流程。如果以nginx作为入口的话，可以在nginx上对每个请求生成一个唯一标识，然后逐级传下去，在各级日志中打印这个标识，即可以在日志中查看到某个请求整个流程的情况。<br>生成方法：<br>####perl方式<br>这种方式较为简单，使用的是nginx的perl模块：ngx_http_perl_module，配置如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perl_set $request_id &#x27;sub &#123;</span><br><span class="line">      return join &quot;&quot;, map&#123;(a..z,A..Z,0..9)[rand 62]&#125; 0..11;</span><br><span class="line">&#125;&#x27;;</span><br></pre></td></tr></table></figure>
<p>其中$request_id即为单次请求的唯一标识。若要调整标识位数为6，即将最后的0..11换为0..6</p>
<p>####其它模块<br><a target="_blank" rel="noopener" href="https://github.com/newobj/nginx-x-rid-header">nginx-x-rid-header</a><br><a target="_blank" rel="noopener" href="https://github.com/hhru/nginx_requestid">nginx_requestid</a></p>
<p>##动态加载模块<br>在nginx1.9之前，模块与nginx一起编译到二进制文件中，使用时加载所有模块运行的。<br>1.9及之后版本，nginx支持动态模块加载，即编译nginx时可使用*_module&#x3D;dynamic的形式，让模块作为动态加载使用。<br>使用动态模块时，需使用load_module path&#x2F;to&#x2F;module，加载模块。</p>
<p>##关于upstream长连接设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">upstream api-open&#123;</span><br><span class="line">    server 127.0.0.1:3000 max_fails=0;</span><br><span class="line">    keepalive 200;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于upstream长连接配置，一般会有这样的误区，认为keepalive表示最大的长连接数，或者最小的长连接数。<br>其实都是错误的。<br>nginx默认是开启长连接的，即每个请求都会分配长连接，这里也不得不提另外一个参数，即允许多少个请求复用同一个长连接，因为这可以减少长连接数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keepalive_requests 100;</span><br></pre></td></tr></table></figure>
<p>然而，upstream里的keepalive并不是限定长连接数的最大或者最小值。而是表示当前活跃的长连接数的限定值。<br>比如，当前有1000的请求进来，nginx需要开启1000个长连接（假如keepalive_requests为0），处理完1000个请求后，由于keepalive为200，所以另外800个长连接会释放掉，当第二次1000个请求进来时，再重新建立800个长连接。<br>所以当并发高时，这个值就设置大的数值，防止长连接不断释放与建立。</p>
<p>##参考<br><a target="_blank" rel="noopener" href="http://www.ttlsa.com/nginx/nginx-limited-connection-number-ngx_http_limit_conn_module-module/">nginx限制连接数ngx_http_limit_conn_module模块</a><br><a target="_blank" rel="noopener" href="https://skyao.gitbooks.io/leaning-nginx/content/documentation/keep_alive.html">支持keep alive长连接</a><br><a target="_blank" rel="noopener" href="http://tshare365.com/archives/2420.html">Nginx总算支持动态模块了</a></p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2015 - 2022 By 杨润炜</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>