---
title: 动手学深度学习（笔记）- 监督学习之多类逻辑回归
date: 2018/02/13 09:33:59
last_updated: 2018/02/13 09:36:07
online_time: 2018/02/13 09:34:02
description: 监督学习之多类逻辑回归
categories:
  - tech
tags:
  - ml
---

# 多类逻辑回归
## 学习内容
- 1.多类逻辑回归-从0开始
- 2.多类逻辑回归-使用Gluon

## 我的理解
- 基本训练流程与上一节的线性回归一样，不同之处在于前者输出结果是多维度的，而后者是单个维度的，前者也有不同的损失函数和输出前的处理等区别。下面主要列举两者的不同之处：
    softmax函数：用在对输出结果的处理，将k维向量转换到另一个k维向量上，使得所有维度都在[0,1]之间，并且维度之和为1，这样就将输出结果归一化为各个类别的概率值；
    交叉熵损失函数：由于softmax使线性模型的参数偏导在大部分时候取值过小，训练时参数更新过于缓慢，所以不使用先前讨论的方差损失函数，而使用交叉熵的形式；当训练结果的概率分布与实际概率分布越相近，代价值越小；

- 提供了优化Softmax和交叉熵数值计算不稳定的函数；不需要对输入数据及参数作维度定义，只需要调出相应的模型层（线性模型用Dense层）来接收数据输入即可；与上节不同的是，此处输入数据是多维矩阵，需要用Flatten层将数据转成特定维度的矩阵。

## 意义
了解了多分类逻辑回归用到的softmax和交叉熵，这些在神经网络中也会经常被用到。

## 参考
[动手学深度学习-多类逻辑回归](https://zh.gluon.ai/chapter_supervised-learning/softmax-regression-scratch.html)
