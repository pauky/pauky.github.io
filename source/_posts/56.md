---
title: mongodb import large file
date: 2016/05/31 11:47:01
last_updated: 2016/05/31 11:47:09
online_time: 2016/05/31 11:47:09
description: 如何利用mongoimport 导入大文件数据
categories:
  - tech
tags:
  - mongodb
---

利用mongodb自带的mongoimport工具，可以向mongodb导入数据，格式可以是json或者csv。
可以看下如下示例命令：
```
mongoimport --db test --collection data --file data.json --authenticationDatabase oo --password abc123 --username user
```
具体参数可参考mongoimport的文档（见参考部分链接）。

## 导入大文件时报错
上述命令一般可以满足需求。然而最近有个需求，需要导入1G以上的数据到mongodb。起初用的还是上述命令，没加其它的限制参数，结果总是出现如下的错误提示：
```
Failed: lost connection to server
```

## 两种解决方案
##### -j 改变并发执行数
网上搜索的结果，大概有两种方案，一种是加“-j”参数，提高并发执行数量。这种方式十分占内存，而且如果设置过大，很有可能造成mongodb无法响应其它操作。一般不推荐使用。
具体命令如下：
```
mongoimport --db test --collection data --file data.json --authenticationDatabase oo --password abc123 --username user -j 8
```

##### --batchSize 改变批量操作数
最后我选择了“--batchSize”参数，默认数值是10000，需要把调小，可以根据实际情况调小这个数值。实践中，我把它设置成1，可能是因为我单条数据比较大，所以设置成这么小才成功导入了数据。
具体示例命令如下：
```
mongoimport --db test --collection data --file data.json --authenticationDatabase oo --password abc123 --username user --batchSize 1
```

## 参考
[mongoimport doc](https://docs.mongodb.com/manual/reference/program/mongoimport/)
[mongoimport batchSize](https://jira.mongodb.org/browse/TOOLS-939)
[MongoDB: mongoimport loses connection when importing big files](http://stackoverflow.com/questions/33475505/mongodb-mongoimport-loses-connection-when-importing-big-files)
