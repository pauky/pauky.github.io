---
title: 《机器学习实战》（笔记）— 理解和实现决策树
date: 2018/02/21 10:22:31
last_updated: 2018/02/21 10:22:34
online_time: 2018/02/21 10:22:34
description: 理解和实现决策树
categories:
  - tech
tags:
  - ml
---

# 决策树
## 学习内容
- 1.了解信息增益
- 2.理解决策树原理
- 3.决策树的简单实现

## 我的理解
- 信息增益是信息论中的熵，表示信息的期望值。
    类别C是变量，它可能的取值是C1，C2，……，Cn，而每一个类别出现的概率是P(C1)，P(C2)，……，P(Cn)，因此n就是类别的总数。此时分类系统的熵就可以表示为：
    ![](https://pic002.cnblogs.com/images/2012/329131/2012052716580941.jpg)
    
- 决策树结构类似于数据结构里的二叉树，通常用来做一些分类任务，样本输入后，经过分支到达叶子节点，该节点代表的分类就是样本数据的识别类别。训练就是构造决策树的过程，节点是由特征来决定的，而特征选用的顺序需要由信息增益（即信息论中的熵）来决定，哪个特征能使信息增益增大（即熵值减小），就先使用该特征作为分支条件，这样递归下去就得到了整棵决策树。

- 创建数据集：每个样本都是一个三维数组，前两维分别表示两种特征的数值，最后一个元素表示此样本所属类别。代码略。
    计算数据集的信息熵：
    ```python
    def calcShannonEnt(dataSet):
        # 数据集总样本数量
        numEntries = len(dataSet)
        # 计算数据集里各个类别的样本数量
        labelCounts = {}
        for featVec in dataSet:
            currentLabel = featVec[-1]
            if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
            labelCounts[currentLabel] += 1
        shannonEnt = 0.0
        # 根据公式计算些数据集的信息熵
        for key in labelCounts:
            prob = float(labelCounts[key])/numEntries
            shannonEnt -= prob * log(prob,2)
        return shannonEnt
    ```
    划分节点的数据集：
    ```python
    def splitDataSet(dataSet, axis, value):
        retDataSet = []
        # 遍历父数据集，找出axis对应的分类value数据，返回新的子节点数据集
        for featVec in dataSet:
            if featVec[axis] == value:
                reducedFeatVec = featVec[:axis]
                reducedFeatVec.extend(featVec[axis+1:])
                retDataSet.append(reducedFeatVec)
        return retDataSet
    ```
    选择最好的数据集划分方式：
    ```python
    def chooseBestFeatureToSplit(dataSet):
        # 计算数据集的特征数量
        numFeatures = len(dataSet[0]) - 1
    
        # 计算数据集的信息熵
        baseEntropy = calcShannonEnt(dataSet)
    
        bestInfoGain = 0.0; bestFeature = -1
        for i in range(numFeatures):
            # 创建具有某特征的值列表，且去重
            featList = [example[i] for example in dataSet]
            uniqueVals = set(featList)
            newEntropy = 0.0
            for value in uniqueVals:
                # 划分特征节点的数据集
                subDataSet = splitDataSet(dataSet, i, value)
                # 计算此特征节点数据集的信息熵
                prob = len(subDataSet)/float(len(dataSet))
                newEntropy += prob * calcShannonEnt(subDataSet)     
            infoGain = baseEntropy - newEntropy
            # 拿到最小的信息熵，并记录该特征
            if (infoGain > bestInfoGain):
                bestInfoGain = infoGain
                bestFeature = i
        # 返回能使数据集信息熵最小的特征
        return bestFeature
    ```
    递归构建决策树：
    ```python
    def createTree(dataSet,labels):
        classList = [example[-1] for example in dataSet]
        # 类别完全相同则直接返回该类别
        if classList.count(classList[0]) == len(classList): 
            return classList[0]
        # 遍历完所有特征时返回出现次数最多的类别
        if len(dataSet[0]) == 1:
            return majorityCnt(classList)
        # 获取最佳节点特征并赋给决策树节点
        bestFeat = chooseBestFeatureToSplit(dataSet)
        bestFeatLabel = labels[bestFeat]
        myTree = {bestFeatLabel:{}}
        del(labels[bestFeat])
        # 得到列表包含的所有特征值，并进行递归构建决策树
        featValues = [example[bestFeat] for example in dataSet]
        uniqueVals = set(featValues)
        for value in uniqueVals:
            subLabels = labels[:]
            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
        return myTree
    ```
    运行结果：
    ```
    >>> import trees
    >>> myDat, labels = trees.createDataSet()
    >>> myTree = trees.createTree(myDat, labels)
    >>> myTree
    {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
    ```
    完整代码请查看：[决策树的实现](https://github.com/pauky/machine_learning_in_action/blob/master/Ch03/trees.py)
## 意义
了解了信息熵的定义和计算公式，对机器学习的分类算法决策树有了一定的理解。
