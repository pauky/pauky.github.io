---
title: 《机器学习实战》（笔记）— 朴素贝叶斯
date: 2018/02/22 10:45:26
last_updated: 2018/02/22 10:47:32
online_time: 2018/02/22 10:45:30
description: 朴素贝叶斯
categories:
  - tech
tags:
  - ml
---

# 朴素贝叶斯
# 学习内容
- 1.了解贝叶斯公式及作用
- 2.实现朴素贝叶斯

# 我的理解
- 贝叶斯是一种基于概率论的分类方法。首先需要了解什么是条件概率，它表示在某条件下事件发生的概率，写作p(c|x)，表示在x条件下，c发生的概率。贝叶斯公式为：
	p(c|x) = p(x|c)p(c) / p(x)
    这个公式告诉我们怎样交换条件概率中的条件与结果。实际场景中我们可以通过数据集知道在各个类别的前提下，特征的概率是多少，这样我们就能利用贝叶斯公式算出在知道样本数据特征的情况下，计算出该样本对应的各个类别的概率，然后取概率最大的作为此数据最可能的类别。
    
- 这里用贝叶斯实现对留言文本进行二分类识别。
    获取数据集：代码略。
    数据向量化：
    ```python
    # 将数据集里出现的词去重后放到一维向量中
    def createVocabList(dataSet):
        vocabSet = set([])
        for document in dataSet:
            vocabSet = vocabSet | set(document)
        return list(vocabSet)

    # 将输入文档inputSet转化为文档向量，此向量的每一元素为0或1，分别表示词汇表中的单词在输入文档中是否出现。
    def setOfWords2Vec(vocabList, inputSet):
        returnVec = [0]*len(vocabList)
        for word in inputSet:
            if word in vocabList:
                returnVec[vocabList.index(word)] = 1
            else: print "the word: %s is not in my Vocabulary!" % word
        return returnVec
    ```
    朴素贝叶斯的训练函数：
    为降低概率为0的影响，将所有词出现的出现数初始为1，计算的分母初始为2；为了防止条件概率的乘积导致数值下溢出为0，需要对乘积结果取自然对数。
    ```python
    def trainNB0(trainMatrix,trainCategory):
        numTrainDocs = len(trainMatrix)
        numWords = len(trainMatrix[0])
        # 因为这里是二分类，所以只计算其中一个分类的概率
        pAbusive = sum(trainCategory)/float(numTrainDocs)
        # 数据集中两个类别的概率向量，即是在某类别发生的情况下，各个词对应的条件概率
        p0Num = ones(numWords); p1Num = ones(numWords)
        p0Denom = 2.0; p1Denom = 2.0
        for i in range(numTrainDocs):
            if trainCategory[i] == 1:
                p1Num += trainMatrix[i]
                p1Denom += sum(trainMatrix[i])
            else:
                p0Num += trainMatrix[i]
                p0Denom += sum(trainMatrix[i])
        p1Vect = log(p1Num/p1Denom)
        p0Vect = log(p0Num/p0Denom)
        return p0Vect,p1Vect,pAbusive
    ```
    朴素贝叶斯分类函数：
    ```python
    def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
        p1 = sum(vec2Classify * p1Vec) + log(pClass1)
        p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
        if p1 > p0:
            return 1
        else: 
            return 0

    def testingNB():
        # 加载数据集
        listOPosts,listClasses = loadDataSet()
        # 数据向量化
        myVocabList = createVocabList(listOPosts)
        trainMat=[]
        for postinDoc in listOPosts:
            trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
        # 计算各类别的概率微量
        p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))
        # 测试样本
        testEntry = ['love', 'my', 'dalmation']
        thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
        print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
        testEntry = ['stupid', 'garbage']
        thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
        print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
    ```
    运行结果：
    ```
    >>> import bayes
    >>> bayes.testingNB()
    ['love', 'my', 'dalmation'] classified as:  0
    ['stupid', 'garbage'] classified as:  1
    ```
    完整代码请查看：[朴素贝叶斯](https://github.com/pauky/machine_learning_in_action/blob/master/Ch04/bayes.py)
    
# 意义
了解贝叶斯这一普遍有效的概率论分类方法的原理和实现。
